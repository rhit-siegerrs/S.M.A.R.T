{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba81581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "239daa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found zip files: ['data_Q3_2019.zip', 'data_Q1_2018.zip', 'data_Q3_2020.zip', 'data_Q2_2018.zip', 'data_Q2_2019.zip', 'data_Q2_2017.zip', 'data_2013.zip', 'data_Q1_2016.zip', 'data_Q4_2021.zip', 'data_Q3_2016.zip', 'data_Q2_2020.zip', 'data_2014.zip', 'data_2015.zip', 'data_Q4_2020.zip', 'data_Q4_2017.zip', 'data_Q1_2019.zip', 'data_Q4_2019.zip', 'data_Q1_2024.zip', 'data_Q3_2017.zip', 'data_Q2_2021.zip', 'data_Q3_2018.zip', 'data_Q1_2017.zip', 'data_Q2_2016.zip', 'data_Q1_2021.zip', 'data_Q1_2020.zip', 'data_Q4_2018.zip', 'data_Q4_2016.zip', 'data_Q3_2021.zip']\n",
      "Extracted data_Q3_2019.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q3_2019\n",
      "Extracted data_Q1_2018.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q1_2018\n",
      "Extracted data_Q3_2020.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q3_2020\n",
      "Extracted data_Q2_2018.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q2_2018\n",
      "Extracted data_Q2_2019.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q2_2019\n",
      "Extracted data_Q2_2017.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q2_2017\n",
      "Extracted data_2013.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_2013\n",
      "Extracted data_Q1_2016.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q1_2016\n",
      "Extracted data_Q4_2021.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q4_2021\n",
      "Extracted data_Q3_2016.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q3_2016\n",
      "Extracted data_Q2_2020.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q2_2020\n",
      "Extracted data_2014.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_2014\n",
      "Extracted data_2015.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_2015\n",
      "Extracted data_Q4_2020.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q4_2020\n",
      "Extracted data_Q4_2017.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q4_2017\n",
      "Extracted data_Q1_2019.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q1_2019\n",
      "Extracted data_Q4_2019.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q4_2019\n",
      "Extracted data_Q1_2024.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q1_2024\n",
      "Extracted data_Q3_2017.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q3_2017\n",
      "Extracted data_Q2_2021.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q2_2021\n",
      "Extracted data_Q3_2018.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q3_2018\n",
      "Extracted data_Q1_2017.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q1_2017\n",
      "Extracted data_Q2_2016.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q2_2016\n",
      "Extracted data_Q1_2021.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q1_2021\n",
      "Extracted data_Q1_2020.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q1_2020\n",
      "Extracted data_Q4_2018.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q4_2018\n",
      "Extracted data_Q4_2016.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q4_2016\n",
      "Extracted data_Q3_2021.zip -> /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_Q3_2021\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "base_dir = Path(\"datasets\").resolve()\n",
    "\n",
    "zip_files = list(base_dir.glob(\"*.zip\"))\n",
    "print(\"Found zip files:\", [z.name for z in zip_files])\n",
    "\n",
    "for zf in zip_files:\n",
    "    out_dir = base_dir / zf.stem   # use the zip file name without .zip\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    with zipfile.ZipFile(zf, 'r') as z:\n",
    "        z.extractall(out_dir)\n",
    "        print(f\"Extracted {zf.name} -> {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bc39fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV files: 996\n",
      "Example: /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_2013/2013/2013-04-10.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# base_dir = Path(\"datasets\")\n",
    "base_dir = Path(\"/work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets\")\n",
    "\n",
    "# 22 - 25\n",
    "# folders = [base_dir / \"data_Q4_2022\", base_dir / \"data_Q4_2023\",base_dir / \"data_Q4_2024\"] # Q4\n",
    "# folders = [base_dir / \"data_Q3_2024\", base_dir / \"data_Q3_2025\",base_dir / \"data_Q3_2023\", base_dir / \"data_Q3_2022\"] # Q3\n",
    "# folders = [base_dir / \"data_Q2_2022\", base_dir / \"data_Q2_2023\",base_dir / \"data_Q2_2024\", base_dir / \"data_Q2_2025\"] # Q2\n",
    "# folders = [base_dir / \"data_Q1_2022\", base_dir / \"data_Q1_2023\",base_dir / \"data_Q1_2024\", base_dir / \"data_Q1_2025\"] # Q1\n",
    "\n",
    "# 16-21\n",
    "# folders = [base_dir / \"data_Q4_2016\", base_dir / \"data_Q4_2017\",base_dir / \"data_Q4_2018\", base_dir / \"data_Q4_2019\", base_dir / \"data_Q4_2020\", base_dir / \"data_Q4_2021\"] # Q4\n",
    "# folders = [base_dir / \"data_Q3_2016\", base_dir / \"data_Q3_2017\",base_dir / \"data_Q3_2018\", base_dir / \"data_Q3_2019\", base_dir / \"data_Q3_2020\", base_dir / \"data_Q3_2021\"] # Q3\n",
    "# folders = [base_dir / \"data_Q2_2016\", base_dir / \"data_Q2_2017\",base_dir / \"data_Q2_2018\", base_dir / \"data_Q2_2019\", base_dir / \"data_Q2_2020\", base_dir / \"data_Q2_2021\"] # Q2\n",
    "# folders = [base_dir / \"data_Q1_2016\", base_dir / \"data_Q1_2017\",base_dir / \"data_Q1_2018\", base_dir / \"data_Q1_2019\", base_dir / \"data_Q1_2020\", base_dir / \"data_Q1_2021\"] # Q1\n",
    "\n",
    "# 13-15\n",
    "folders = [base_dir / \"data_2013\"/\"2013\", base_dir / \"data_2014\"/\"2014\",base_dir / \"data_2015\"/\"2015\"] # Q1\n",
    "\n",
    "csv_files = []\n",
    "for f in folders:\n",
    "    if not f.exists():\n",
    "        raise FileNotFoundError(f\"Missing folder: {f.resolve()}\")\n",
    "    csv_files.extend(sorted(f.glob(\"*.csv\")))\n",
    "\n",
    "print(\"Found CSV files:\", len(csv_files))\n",
    "print(\"Example:\", csv_files[0] if csv_files else \"NONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3b625756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 266 files\n",
      "2014: 365 files\n",
      "2015: 365 files\n"
     ]
    }
   ],
   "source": [
    "for f in folders:\n",
    "    files = sorted(f.glob(\"*.csv\"))\n",
    "    print(f\"{f.name}: {len(files)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e51ef160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using processes: 128\n",
      "Loaded files: 996\n",
      "Seconds: 17.655\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "try:\n",
    "    cpu_workers = len(os.sched_getaffinity(0))\n",
    "except Exception:\n",
    "    cpu_workers = os.cpu_count() or 1\n",
    "\n",
    "# Use half of the cores, or you will be kicked out of the server!!!!!\n",
    "max_workers = min(cpu_workers, len(csv_files)) if csv_files else cpu_workers\n",
    "max_workers = max_workers//2\n",
    "print(\"Using processes:\", max_workers)\n",
    "\n",
    "def _read_one_csv(abs_path: str, base_dir_str: str):\n",
    "    \"\"\"\n",
    "    Executed inside a worker process: reads one CSV file and returns (key, DataFrame).\n",
    "    Note: the function must be defined at the top level so it can be pickled\n",
    "    by ProcessPoolExecutor.\n",
    "    \"\"\"\n",
    "    p = Path(abs_path)\n",
    "    base = Path(base_dir_str)\n",
    "\n",
    "    # key: relative path from base_dir\n",
    "    key = str(p.relative_to(base))\n",
    "\n",
    "    df = pd.read_csv(p, low_memory=False)\n",
    "\n",
    "    if \"failure\" in df.columns:\n",
    "        df = df[df[\"failure\"] == 1]\n",
    "    else:\n",
    "        # If no 'failure' column, return empty DataFrame with same columns\n",
    "        df = df.iloc[0:0]\n",
    "    return key, df\n",
    "\n",
    "t0 = time.time()\n",
    "all_data = {}\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "    futures = [\n",
    "        ex.submit(_read_one_csv, str(p.resolve()), str(base_dir.resolve()))\n",
    "        for p in csv_files\n",
    "    ]\n",
    "    for fut in as_completed(futures):\n",
    "        key, df = fut.result()\n",
    "        all_data[key] = df\n",
    "\n",
    "print(\"Loaded files:\", len(all_data))\n",
    "print(\"Seconds:\", round(time.time() - t0, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bdbc21d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows with failure == 1: 4346\n"
     ]
    }
   ],
   "source": [
    "total_rows = sum(len(df) for df in all_data.values())\n",
    "\n",
    "print(\"Total rows with failure == 1:\", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ead87f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2050799/1750701054.py:2: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat(all_data.values(), axis=0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged failure==1 rows to: merged_13-15_failure1.csv\n",
      "Total rows written: 4346\n"
     ]
    }
   ],
   "source": [
    "# Merge all DataFrames into one, how = 'outer' to include all columns\n",
    "merged_df = pd.concat(all_data.values(), axis=0, ignore_index=True)\n",
    "\n",
    "out_file = \"merged_13-15_failure1.csv\"\n",
    "\n",
    "merged_df.to_csv(out_file, index=False)\n",
    "\n",
    "print(\"Saved merged failure==1 rows to:\", out_file)\n",
    "print(\"Total rows written:\", len(merged_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4e92cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             date serial_number                 model  capacity_bytes  failure  \\\n",
       "0     2022-04-02      ZGS01JWR            ST500LM030    500107862016        1   \n",
       "1     2022-04-02      ZHZ62HSV         ST12000NM0008  12000138625024        1   \n",
       "2     2022-04-02      ZHZ65PAT         ST12000NM0008  12000138625024        1   \n",
       "3     2022-04-02  X880A0H9F97G   TOSHIBA MG07ACA14TA  14000519643136        1   \n",
       "4     2022-04-02      ZL2CH3NJ         ST14000NM001G  14000519643136        1   \n",
       "...          ...           ...                   ...             ...      ...   \n",
       "4471  2025-06-17      2BJD0GDD   WDC WUH721816ALE6L0  16000900661248        1   \n",
       "4472  2025-06-17  8190A0GUFV8G  TOSHIBA MG08ACA16TEY  16000900661248        1   \n",
       "4473  2025-06-17  8190A1LGFV8G  TOSHIBA MG08ACA16TEY  16000900661248        1   \n",
       "4474  2025-06-17  44M0B03YV4MJ   TOSHIBA MG10ACA20TE  20000588955648        1   \n",
       "4475  2025-06-17      ZYD1CGB1         ST24000NM002H  24000277250048        1   \n",
       "\n",
       "      smart_1_normalized  smart_1_raw  smart_2_normalized  smart_2_raw  \\\n",
       "0                   83.0  191820128.0                 NaN          NaN   \n",
       "1                   58.0  227791144.0                 NaN          NaN   \n",
       "2                   80.0   94048872.0                 NaN          NaN   \n",
       "3                  100.0          0.0               100.0          0.0   \n",
       "4                   83.0  210490304.0                 NaN          NaN   \n",
       "...                  ...          ...                 ...          ...   \n",
       "4471               100.0          0.0               100.0         96.0   \n",
       "4472               100.0          0.0               100.0          0.0   \n",
       "4473               100.0          0.0               100.0          0.0   \n",
       "4474               100.0          0.0               100.0          0.0   \n",
       "4475                83.0  203779473.0                 NaN          NaN   \n",
       "\n",
       "      smart_3_normalized  ...  cluster_id  pod_slot_num  smart_27_normalized  \\\n",
       "0                   99.0  ...         NaN           NaN                  NaN   \n",
       "1                   97.0  ...         NaN           NaN                  NaN   \n",
       "2                   95.0  ...         NaN           NaN                  NaN   \n",
       "3                  100.0  ...         NaN           NaN                  NaN   \n",
       "4                   91.0  ...         NaN           NaN                  NaN   \n",
       "...                  ...  ...         ...           ...                  ...   \n",
       "4471                84.0  ...        31.0           NaN                  NaN   \n",
       "4472               100.0  ...        31.0           NaN                  NaN   \n",
       "4473               100.0  ...        31.0           NaN                  NaN   \n",
       "4474               100.0  ...        50.0           NaN                100.0   \n",
       "4475                93.0  ...        50.0           NaN                  NaN   \n",
       "\n",
       "      smart_27_raw  smart_82_normalized  smart_82_raw  smart_211_normalized  \\\n",
       "0              NaN                  NaN           NaN                   NaN   \n",
       "1              NaN                  NaN           NaN                   NaN   \n",
       "2              NaN                  NaN           NaN                   NaN   \n",
       "3              NaN                  NaN           NaN                   NaN   \n",
       "4              NaN                  NaN           NaN                   NaN   \n",
       "...            ...                  ...           ...                   ...   \n",
       "4471           NaN                  NaN           NaN                   NaN   \n",
       "4472           NaN                  NaN           NaN                   NaN   \n",
       "4473           NaN                  NaN           NaN                   NaN   \n",
       "4474      329490.0                  NaN           NaN                   NaN   \n",
       "4475           NaN                  NaN           NaN                   NaN   \n",
       "\n",
       "      smart_211_raw  smart_212_normalized  smart_212_raw  \n",
       "0               NaN                   NaN            NaN  \n",
       "1               NaN                   NaN            NaN  \n",
       "2               NaN                   NaN            NaN  \n",
       "3               NaN                   NaN            NaN  \n",
       "4               NaN                   NaN            NaN  \n",
       "...             ...                   ...            ...  \n",
       "4471            NaN                   NaN            NaN  \n",
       "4472            NaN                   NaN            NaN  \n",
       "4473            NaN                   NaN            NaN  \n",
       "4474            NaN                   NaN            NaN  \n",
       "4475            NaN                   NaN            NaN  \n",
       "\n",
       "[4476 rows x 197 columns]>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(out_file, low_memory=False)\n",
    "df.head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
