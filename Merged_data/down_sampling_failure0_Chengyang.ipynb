{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a21a2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV files: 996\n",
      "Example: /work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets/data_2013/2013/2013-04-10.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# base_dir = Path(\"datasets\")\n",
    "base_dir = Path(\"/work/shibberu/share/MA384_Data_Mining_Projects_Winter_2025-26/SMART_failure_prediction/S.M.A.R.T/datasets\")\n",
    "\n",
    "# 22 - 25\n",
    "# folders = [base_dir / \"data_Q4_2022\", base_dir / \"data_Q4_2023\",base_dir / \"data_Q4_2024\"] # Q4\n",
    "# folders = [base_dir / \"data_Q3_2024\", base_dir / \"data_Q3_2025\",base_dir / \"data_Q3_2023\", base_dir / \"data_Q3_2022\"] # Q3\n",
    "# folders = [base_dir / \"data_Q2_2022\", base_dir / \"data_Q2_2023\",base_dir / \"data_Q2_2024\", base_dir / \"data_Q2_2025\"] # Q2\n",
    "# folders = [base_dir / \"data_Q1_2022\", base_dir / \"data_Q1_2023\",base_dir / \"data_Q1_2024\", base_dir / \"data_Q1_2025\"] # Q1\n",
    "\n",
    "# 16-21\n",
    "# folders = [base_dir / \"data_Q4_2016\", base_dir / \"data_Q4_2017\",base_dir / \"data_Q4_2018\", base_dir / \"data_Q4_2019\", base_dir / \"data_Q4_2020\", base_dir / \"data_Q4_2021\"] # Q4\n",
    "# folders = [base_dir / \"data_Q3_2016\", base_dir / \"data_Q3_2017\",base_dir / \"data_Q3_2018\", base_dir / \"data_Q3_2019\", base_dir / \"data_Q3_2020\", base_dir / \"data_Q3_2021\"] # Q3\n",
    "# folders = [base_dir / \"data_Q2_2016\", base_dir / \"data_Q2_2017\",base_dir / \"data_Q2_2018\", base_dir / \"data_Q2_2019\", base_dir / \"data_Q2_2020\", base_dir / \"data_Q2_2021\"] # Q2\n",
    "# folders = [base_dir / \"data_Q1_2016\", base_dir / \"data_Q1_2017\",base_dir / \"data_Q1_2018\", base_dir / \"data_Q1_2019\", base_dir / \"data_Q1_2020\", base_dir / \"data_Q1_2021\"] # Q1\n",
    "\n",
    "# 13-15\n",
    "folders = [base_dir / \"data_2013\"/\"2013\", base_dir / \"data_2014\"/\"2014\",base_dir / \"data_2015\"/\"2015\"] # full years\n",
    "\n",
    "csv_files = []\n",
    "for f in folders:\n",
    "    if not f.exists():\n",
    "        raise FileNotFoundError(f\"Missing folder: {f.resolve()}\")\n",
    "    csv_files.extend(sorted(f.glob(\"*.csv\")))\n",
    "\n",
    "print(\"Found CSV files:\", len(csv_files))\n",
    "print(\"Example:\", csv_files[0] if csv_files else \"NONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac2fb0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013: 266 files\n",
      "2014: 365 files\n",
      "2015: 365 files\n"
     ]
    }
   ],
   "source": [
    "for f in folders:\n",
    "    files = sorted(f.glob(\"*.csv\"))\n",
    "    print(f\"{f.name}: {len(files)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f05fdb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using processes: 128\n",
      "Candidate rows: 124027 from files: 712\n",
      "Candidate unique models: 84\n",
      "Seconds (read candidates): 17.847\n",
      "Final rows: 83917\n",
      "Final unique models: 84\n",
      "Model distribution (top 20):\n",
      "model\n",
      "ST320LT007                 1191\n",
      "ST4000DM000                1191\n",
      "Hitachi HDS5C3030ALA630    1191\n",
      "WDC WD1600AAJS             1191\n",
      "WDC WD1600BPVT             1191\n",
      "                           ... \n",
      "WDC WD10EALS                252\n",
      "Hitachi HDT721010SLA360     217\n",
      "WDC WD30EZRS                148\n",
      "WDC WD2500JB                 38\n",
      "SAMSUNG HD103UJ              33\n",
      "Name: count, Length: 84, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time\n",
    "\n",
    "# cores and threads setting\n",
    "try:\n",
    "    cpu_workers = len(os.sched_getaffinity(0))\n",
    "except Exception:\n",
    "    cpu_workers = os.cpu_count() or 1\n",
    "\n",
    "max_workers = min(cpu_workers, len(csv_files)) if csv_files else cpu_workers\n",
    "max_workers = max_workers // 2\n",
    "max_workers = max(1, max_workers)\n",
    "print(\"Using processes:\", max_workers)\n",
    "\n",
    "\n",
    "TARGET_N = 100000 # this is a threshold for preventing load too many rows for too much RAM usage\n",
    "MODEL_COL = \"model\" # we want to balance rows per model to make the feature dynamic\n",
    "FAIL_COL  = \"failure\"\n",
    "\n",
    "# upper limit of candidates per file to prevent too many data comes from single file\n",
    "CAND_PER_FILE = 300\n",
    "\n",
    "# upper limit of candidates per model per file\n",
    "PER_MODEL_PER_FILE = 3   \n",
    "\n",
    "# filter: minimum non-null fraction per row\n",
    "MIN_NON_NULL_FRAC = 0.25\n",
    "\n",
    "\n",
    "def _read_one_csv(abs_path: str, base_dir_str: str):\n",
    "    \"\"\"\n",
    "     read one csv file and return candidate rows DataFrame\n",
    "    1) filter failure==0\n",
    "    2) filter rows with too many nulls\n",
    "    3) per model, take top PER_MODEL_PER_FILE rows by non-null count\n",
    "    4) finally, if still too many rows, take top CAND_PER_FILE by non-null count\n",
    "    5) add _key column for tracking source file\n",
    "    6) return the candidate DataFrame   \n",
    "    \"\"\"\n",
    "    p = Path(abs_path)\n",
    "    base = Path(base_dir_str)\n",
    "    key = str(p.relative_to(base))\n",
    "\n",
    "    df = pd.read_csv(p, low_memory=False)\n",
    "\n",
    "    # basic check\n",
    "    if FAIL_COL not in df.columns or MODEL_COL not in df.columns:\n",
    "        return pd.DataFrame(columns=[\"_key\", MODEL_COL, \"_nonnull_cnt\", \"_nonnull_frac\"])\n",
    "\n",
    "    # failure == 0\n",
    "    df = df[df[FAIL_COL] == 0]\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"_key\", MODEL_COL, \"_nonnull_cnt\", \"_nonnull_frac\"])\n",
    "\n",
    "    df[MODEL_COL] = df[MODEL_COL].astype(str)\n",
    "    df = df[df[MODEL_COL].notna()]\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"_key\", MODEL_COL, \"_nonnull_cnt\", \"_nonnull_frac\"])\n",
    "\n",
    "    # compute non-null counts and fractions\n",
    "    df[\"_nonnull_cnt\"] = df.notna().sum(axis=1)\n",
    "    df[\"_nonnull_frac\"] = df[\"_nonnull_cnt\"] / df.shape[1]\n",
    "\n",
    "    # filter by non-null fraction\n",
    "    df = df[df[\"_nonnull_frac\"] >= MIN_NON_NULL_FRAC]\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"_key\", MODEL_COL, \"_nonnull_cnt\", \"_nonnull_frac\"])\n",
    "\n",
    "    # per model limit, sampling based on models\n",
    "    df = df.sort_values(\"_nonnull_cnt\", ascending=False)\n",
    "\n",
    "    df = (\n",
    "        df.groupby(MODEL_COL, group_keys=False)\n",
    "          .head(PER_MODEL_PER_FILE)\n",
    "    )\n",
    "\n",
    "    # another global limit per file\n",
    "    if len(df) > CAND_PER_FILE:\n",
    "        df = df.sort_values(\"_nonnull_cnt\", ascending=False).head(CAND_PER_FILE)\n",
    "\n",
    "    df[\"_key\"] = key\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# multiprocessed reading and filtering\n",
    "t0 = time.time()\n",
    "candidates = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "    futures = [\n",
    "        ex.submit(_read_one_csv, str(p.resolve()), str(base_dir.resolve()))\n",
    "        for p in csv_files\n",
    "    ]\n",
    "    for fut in as_completed(futures):\n",
    "        cdf = fut.result()\n",
    "        if cdf is not None and not cdf.empty:\n",
    "            candidates.append(cdf)\n",
    "\n",
    "if not candidates:\n",
    "    raise RuntimeError(\"No candidates found: either no failure==0 rows, or filters are too strict.\")\n",
    "\n",
    "cand_df = pd.concat(candidates, axis=0, ignore_index=True)\n",
    "print(\"Candidate rows:\", len(cand_df), \"from files:\", cand_df[\"_key\"].nunique())\n",
    "print(\"Candidate unique models:\", cand_df[MODEL_COL].nunique())\n",
    "print(\"Seconds (read candidates):\", round(time.time() - t0, 3))\n",
    "\n",
    "\n",
    "groups = {}\n",
    "for m, g in cand_df.groupby(MODEL_COL, sort=False):\n",
    "    groups[m] = g.sort_values(\"_nonnull_cnt\", ascending=False)\n",
    "\n",
    "models = list(groups.keys())\n",
    "num_models = len(models)\n",
    "\n",
    "base_quota = TARGET_N // num_models\n",
    "remainder = TARGET_N % num_models\n",
    "\n",
    "selected_parts = []\n",
    "selected_count = 0\n",
    "leftover_pool = []\n",
    "\n",
    "for i, m in enumerate(models):\n",
    "    g = groups[m]\n",
    "    quota = base_quota + (1 if i < remainder else 0)\n",
    "    take_n = min(quota, len(g))\n",
    "    if take_n > 0:\n",
    "        selected_parts.append(g.head(take_n))\n",
    "        selected_count += take_n\n",
    "    if take_n < len(g):\n",
    "        leftover_pool.append(g.iloc[take_n:])\n",
    "\n",
    "# # \n",
    "# if selected_count < TARGET_N:\n",
    "#     need = TARGET_N - selected_count\n",
    "#     if leftover_pool:\n",
    "#         rest = pd.concat(leftover_pool, axis=0, ignore_index=True)\n",
    "#         rest = rest.sort_values(\"_nonnull_cnt\", ascending=False).head(need)\n",
    "#         selected_parts.append(rest)\n",
    "#         selected_count += len(rest)\n",
    "\n",
    "final_df = pd.concat(selected_parts, axis=0, ignore_index=True)\n",
    "\n",
    "# prevent exceeding TARGET_N\n",
    "if len(final_df) > TARGET_N:\n",
    "    final_df = final_df.sort_values(\"_nonnull_cnt\", ascending=False).head(TARGET_N).reset_index(drop=True)\n",
    "\n",
    "print(\"Final rows:\", len(final_df))\n",
    "print(\"Final unique models:\", final_df[MODEL_COL].nunique())\n",
    "print(\"Model distribution (top 20):\")\n",
    "print(final_df[MODEL_COL].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ca91cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique models: 84\n",
      "Target total: 6000\n",
      "Per-model quota q: 72\n",
      "Sampled total (no backfill): 5975\n",
      "Min/Max per model: 33 72\n",
      "Models below quota: 2\n",
      "\n",
      "Bottom 10 models by count:\n",
      " model\n",
      "SAMSUNG HD103UJ    33\n",
      "WDC WD2500JB       38\n",
      "ST320LT007         72\n",
      "ST250LT007         72\n",
      "ST4000DX000        72\n",
      "ST1500DL003        72\n",
      "ST3000DM001        72\n",
      "ST33000651AS       72\n",
      "ST2000DL003        72\n",
      "ST6000DX000        72\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "MODEL_COL = \"model\"\n",
    "TARGET_TOTAL = 6000   # final wanted total rows written to a csv\n",
    "\n",
    "# sample down based on per-model quota\n",
    "K = final_df[MODEL_COL].nunique()\n",
    "q = int(np.ceil(TARGET_TOTAL / K))   \n",
    "print(\"Unique models:\", K)\n",
    "print(\"Target total:\", TARGET_TOTAL)\n",
    "print(\"Per-model quota q:\", q)\n",
    "\n",
    "# sort by quality: non-null fraction first, then non-null count\n",
    "dfq = final_df.sort_values([\"_nonnull_frac\", \"_nonnull_cnt\"], ascending=[False, False]).copy()\n",
    "\n",
    "# for each model: take up to q rows; if not enough, take all\n",
    "sampled_df = (\n",
    "    dfq.groupby(MODEL_COL, group_keys=False)\n",
    "       .head(q)\n",
    "       .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "vc = sampled_df[MODEL_COL].value_counts()\n",
    "print(\"Sampled total (no backfill):\", len(sampled_df))\n",
    "print(\"Min/Max per model:\", int(vc.min()), int(vc.max()))\n",
    "print(\"Models below quota:\", int((vc < q).sum()))\n",
    "print(\"\\nBottom 10 models by count:\\n\", vc.sort_values().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e454552",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.to_csv(\"merged_13-15_failure0.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f53490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model counts:\n",
      "model\n",
      "ST12000NM0008                             525\n",
      "ST8000NM0055                              462\n",
      "HGST HUH721212ALN604                      452\n",
      "TOSHIBA MG07ACA14TA                       442\n",
      "ST4000DM000                               381\n",
      "HGST HUH721212ALE604                      270\n",
      "ST8000DM002                               243\n",
      "TOSHIBA MG08ACA16TA                       233\n",
      "ST16000NM001G                             215\n",
      "ST14000NM001G                             158\n",
      "ST12000NM001G                             157\n",
      "TOSHIBA MG08ACA16TE                       117\n",
      "ST14000NM0138                              89\n",
      "ST12000NM0007                              83\n",
      "WDC WUH722222ALE6L4                        79\n",
      "WDC WUH721816ALE6L4                        70\n",
      "ST10000NM0086                              67\n",
      "TOSHIBA MG08ACA16TEY                       57\n",
      "WDC WUH721414ALE6L4                        53\n",
      "HGST HMS5C4040BLE640                       48\n",
      "WDC WUH721816ALE6L0                        33\n",
      "TOSHIBA MQ01ABF050                         31\n",
      "TOSHIBA MQ01ABF050M                        28\n",
      "ST24000NM002H                              28\n",
      "TOSHIBA MG10ACA20TE                        20\n",
      "HGST HMS5C4040ALE640                       18\n",
      "ST500LM012 HN                              17\n",
      "HGST HUH728080ALE600                       17\n",
      "HGST HUH721212ALE600                       15\n",
      "ST500LM030                                 12\n",
      "Seagate BarraCuda SSD ZA250CM10002          7\n",
      "TOSHIBA MG07ACA14TEY                        6\n",
      "ST6000DX000                                 5\n",
      "ST12000NM000J                               5\n",
      "WD Blue SA510 2.5 250GB                     5\n",
      "HGST HUH728080ALE604                        4\n",
      "WDC WD5000LPVX                              3\n",
      "TOSHIBA HDWF180                             3\n",
      "Seagate BarraCuda 120 SSD ZA250CM10003      3\n",
      "CT250MX500SSD1                              2\n",
      "ST14000NM0018                               2\n",
      "MTFDDAV240TCB                               1\n",
      "ST4000DM005                                 1\n",
      "ST12000NM0117                               1\n",
      "ST8000NM000A                                1\n",
      "HGST HUS728T8TALE6L4                        1\n",
      "DELLBOSS VD                                 1\n",
      "WUH721816ALE6L4                             1\n",
      "Micron 5300 MTFDDAK480TDS                   1\n",
      "Seagate SSD                                 1\n",
      "ST14000NM000J                               1\n",
      "TOSHIBA MG09ACA16TE                         1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Model ratios:\n",
      "model\n",
      "ST12000NM0008                             0.117292\n",
      "ST8000NM0055                              0.103217\n",
      "HGST HUH721212ALN604                      0.100983\n",
      "TOSHIBA MG07ACA14TA                       0.098749\n",
      "ST4000DM000                               0.085121\n",
      "HGST HUH721212ALE604                      0.060322\n",
      "ST8000DM002                               0.054290\n",
      "TOSHIBA MG08ACA16TA                       0.052055\n",
      "ST16000NM001G                             0.048034\n",
      "ST14000NM001G                             0.035299\n",
      "ST12000NM001G                             0.035076\n",
      "TOSHIBA MG08ACA16TE                       0.026139\n",
      "ST14000NM0138                             0.019884\n",
      "ST12000NM0007                             0.018543\n",
      "WDC WUH722222ALE6L4                       0.017650\n",
      "WDC WUH721816ALE6L4                       0.015639\n",
      "ST10000NM0086                             0.014969\n",
      "TOSHIBA MG08ACA16TEY                      0.012735\n",
      "WDC WUH721414ALE6L4                       0.011841\n",
      "HGST HMS5C4040BLE640                      0.010724\n",
      "WDC WUH721816ALE6L0                       0.007373\n",
      "TOSHIBA MQ01ABF050                        0.006926\n",
      "TOSHIBA MQ01ABF050M                       0.006256\n",
      "ST24000NM002H                             0.006256\n",
      "TOSHIBA MG10ACA20TE                       0.004468\n",
      "HGST HMS5C4040ALE640                      0.004021\n",
      "ST500LM012 HN                             0.003798\n",
      "HGST HUH728080ALE600                      0.003798\n",
      "HGST HUH721212ALE600                      0.003351\n",
      "ST500LM030                                0.002681\n",
      "Seagate BarraCuda SSD ZA250CM10002        0.001564\n",
      "TOSHIBA MG07ACA14TEY                      0.001340\n",
      "ST6000DX000                               0.001117\n",
      "ST12000NM000J                             0.001117\n",
      "WD Blue SA510 2.5 250GB                   0.001117\n",
      "HGST HUH728080ALE604                      0.000894\n",
      "WDC WD5000LPVX                            0.000670\n",
      "TOSHIBA HDWF180                           0.000670\n",
      "Seagate BarraCuda 120 SSD ZA250CM10003    0.000670\n",
      "CT250MX500SSD1                            0.000447\n",
      "ST14000NM0018                             0.000447\n",
      "MTFDDAV240TCB                             0.000223\n",
      "ST4000DM005                               0.000223\n",
      "ST12000NM0117                             0.000223\n",
      "ST8000NM000A                              0.000223\n",
      "HGST HUS728T8TALE6L4                      0.000223\n",
      "DELLBOSS VD                               0.000223\n",
      "WUH721816ALE6L4                           0.000223\n",
      "Micron 5300 MTFDDAK480TDS                 0.000223\n",
      "Seagate SSD                               0.000223\n",
      "ST14000NM000J                             0.000223\n",
      "TOSHIBA MG09ACA16TE                       0.000223\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Total number of unique models: 52\n",
      "Number of columns: 197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2198885/3564807754.py:1: DtypeWarning: Columns (181) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_fail1 = pd.read_csv(\"merged_Q2_22-25_failure1.csv\")\n"
     ]
    }
   ],
   "source": [
    "# df_fail1 = pd.read_csv(\"merged_Q2_22-25_failure1.csv\")\n",
    "\n",
    "# model_counts = df_fail1[\"model\"].value_counts()\n",
    "# model_ratio = df_fail1[\"model\"].value_counts(normalize=True)\n",
    "\n",
    "# print(\"Model counts:\")\n",
    "# print(model_counts)\n",
    "\n",
    "# print(\"\\nModel ratios:\")\n",
    "# print(model_ratio)\n",
    "\n",
    "# print(\"\\nTotal number of unique models:\", df_fail1[\"model\"].nunique())\n",
    "\n",
    "# print(\"Number of columns:\", df_fail1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0ec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: 18\n",
      "   merged_13-15_failure0.csv\n",
      "   merged_13-15_failure1.csv\n",
      "   merged_Q1_16-21_failure1.csv\n",
      "   merged_Q1_16_21_failure0.csv\n",
      "   merged_Q1_22-25_failure1.csv\n",
      "   merged_Q1_22_25_failure0.csv\n",
      "   merged_Q2_16-21_failure1.csv\n",
      "   merged_Q2_16_21_failure0.csv\n",
      "   merged_Q2_22-25_failure1.csv\n",
      "   merged_Q2_22_25_failure0.csv\n",
      "   merged_Q3_16-21_failure1.csv\n",
      "   merged_Q3_16_21_failure0.csv\n",
      "   merged_Q3_22-25_failure1.csv\n",
      "   merged_Q3_22_25_failure0.csv\n",
      "   merged_Q4_16-21_failure1.csv\n",
      "   merged_Q4_16_21_failure0.csv\n",
      "   merged_Q4_22-25_failure1.csv\n",
      "   merged_Q4_22_25_failure0.csv\n",
      "Merged shape: (76542, 197)\n",
      "Saved (auxiliary columns removed)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "base_dir = Path(\".\")\n",
    "pattern = \"merged_*_failure*.csv\"\n",
    "\n",
    "csv_files = sorted(base_dir.glob(pattern))\n",
    "print(\"Found files:\", len(csv_files))\n",
    "for f in csv_files:\n",
    "    print(\"  \", f.name)\n",
    "\n",
    "dfs = []\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f, low_memory=False)\n",
    "    df[\"_source_file\"] = f.name  \n",
    "    dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(dfs, axis=0, join=\"outer\", ignore_index=True)\n",
    "\n",
    "# delete auxiliary columns used during processing\n",
    "cols_to_drop = [\"_nonnull_cnt\", \"_nonnull_frac\", \"_key\", \"_source_file\"]\n",
    "merged_df = merged_df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "print(\"Merged shape:\", merged_df.shape)\n",
    "merged_df.to_csv(\"merged_all.csv\", index=False)\n",
    "\n",
    "print(\"Saved (auxiliary columns removed)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d70bc",
   "metadata": {},
   "source": [
    "# Check merged files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7faba309",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_csv(\"merged_all.csv\", low_memory=False)\n",
    "# print all columns to a txt file\n",
    "with open(\"merged_all_columns.txt\", \"w\") as f:\n",
    "    for col in df_merged.columns:\n",
    "        f.write(col + \"\\n\") \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e6db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique SMART IDs: 93\n",
      "No SMART ID appears more than twice (raw + normalized only).\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "with open(\"merged_all_columns.txt\", \"r\") as f:\n",
    "    cols = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# get SMART IDs\n",
    "smart_nums = []\n",
    "for c in cols:\n",
    "    m = re.match(r\"smart_(\\d+)_\", c)\n",
    "    if m:\n",
    "        smart_nums.append(int(m.group(1)))\n",
    "\n",
    "cnt = Counter(smart_nums)\n",
    "\n",
    "# filter appearances >= 3 (more than raw + normalized)\n",
    "dup = {k: v for k, v in cnt.items() if v >= 3}\n",
    "\n",
    "print(\"Total unique SMART IDs:\", len(cnt))\n",
    "\n",
    "if dup:\n",
    "    print(\"SMART IDs appearing more than twice:\")\n",
    "    for k, v in sorted(dup.items()):\n",
    "        print(f\"smart_{k}: {v} times\")\n",
    "else:\n",
    "    print(\"No SMART ID appears more than twice (raw + normalized only).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6479cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with failure==0: 46416\n",
      "Number of rows with failure==1: 30126\n"
     ]
    }
   ],
   "source": [
    "# get number of rows for failure==0 and failure==1\n",
    "df_merged = pd.read_csv(\"merged_all.csv\", low_memory=False)\n",
    "num_fail0 = (df_merged[\"failure\"] == 0).sum()\n",
    "num_fail1 = (df_merged[\"failure\"] == 1).sum()\n",
    "print(\"Number of rows with failure==0:\", num_fail0)\n",
    "print(\"Number of rows with failure==1:\", num_fail1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
